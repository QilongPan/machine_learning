#数据挖掘教程
## 数据挖掘流程
### 数据获取
- 从第三方直接获得  
- 通过爬虫爬取  
### 数据保存形式  
- 数据库(mysql等)
- 集群(hadoop)
- 文本文件(txt,csv,json等)  
### 数据处理工具  
- pandas可以读取csv,json，sql，excel,pickling等数据，并且可以对数据做各种处理。

- hive,spark可以将集群中数据存储为csv格式

  
## EDA  

1.通过pandas中describe(),info(),head()等函数查看数据。

2.缺失值查看。

3.相关系数查看

```
#correlation matrix
corrmat = train.corr()
f, ax = plt.subplots(figsize=(20, 9))
sns.heatmap(corrmat, vmax=.8, annot=True);
```

4.使用pandas中的get_dummies标签化数据one hot encoder

5.回归问题如果目标具有长尾效应，则可以采用np.log1p()进行对数变换。



### 处理和清洗

**数据规约**：数据规约技术可以用来得到数据集的归约表示，数据要小很多，但数据的完整性与原数据接近。 数据规约是通过聚集、删除冗余特征或者聚类的方法来压缩数据。   
**数据规约技术**：降维(奇异值分解(SVD)、主成分分析(PCA)、因子分析(FA)、独立成分分析(ICA))。  
**维归约**：删除不相关的特征或维，减少数据量。通常使用属性子集的选择方法，即特征提取。

1.数据清洗方法  
**探索性分析**：使用科学计算库(numpy,pandas,matplotlib等)探索数据，查看数据的数据类型、缺失值、数据集规模、各特征下的数据分布情况等。还可以通过单变量分析与多变量分析，查看各特征之间的关系，以验证业务分析阶段所提出的假设。  
**缺失值**：缺失值可以通过如下方法处理：  
(1)在缺失率少且属性重要程度低的情况下，若属性为数值型数据则根据数据分布情况简单的填充即可，例如：若数据分布均匀，则使用均值对数据进行填充即可；若数据分布倾斜，使用中位数填充即可。若属性为类别属性，则可以用一个全局常量‘Unknow’填充，但是，这样做往往效果很差，因为算法可能会将其识别为一个全新的类别，因此很少使用。  
(2)当缺失率高(>95%)且属性重要程度低时，直接删除该属性即可。然而在缺失值高且属性程度较高时，直接删除该属性对于算法的结果会造成很不好的影响。  
(3)缺失值高，属性重要程度高：主要使用的方法有插补法与建模法。  
a 随机插补法--从总体中随机抽取某几个样本代替缺失样本。  
b 多重插补法--通过变量之间的关系对缺失数据进行预测，利用蒙特卡洛方法生成多个完整的数据集，在对这些数据集进行分析，最后对分析结果进行汇总处理。  
c 热平台插补----指在非缺失数据集中找到一个与缺失值所在样本相似的样本（匹配样本），利用其中的观测值对缺失值进行插补。  
d 建模法：可以用回归、贝叶斯、随机森林、决策树等模型对缺失数据进行预测。例如：利用数据集中其他数据的属性，可以构造一棵判定树，来预测缺失值的值。  
**异常值**：
检测异常值方法:  
a 箱形图  
b 简单的统计分析  
c 基于正态分布的离群点检测  
d 基于模型检测:首先建立一个数据模型，异常是那些同模型不能完美拟合的对象；如果模型是簇的集合，则异常是不显著属于任何簇的对象；在使用回归模型时，异常是相对远离预测值的对象。  
e 基于距离:通过在对象之间定义临近性度量，异常对象是那些远离其它对象的对象。
f 基于密度：当一个点的局部密度显著低于它的大部分近邻时才将其分类为离群点。适合非均匀分布的数据。
g 基于聚类：基于聚类的离群点：一个对象是基于聚类的离群点，如果该对象不强属于任何簇。离群点对初始聚类的影响：如果通过聚类检测离群点，则由于离群点影响聚类，存在一个问题：结构是否有效。为了处理该问题，可以使用如下方法：对象聚类，删除离群点，对象再次聚类。  
处理异常点方法：  
a 删除异常值----明显看出是异常且数量较少可以直接删除  
b 不处理---如果算法对异常值不敏感则可以不处理，但如果算法对异常值敏感，则最好不要用这种方法，如基于距离计算的一些算法，包括kmeans，knn之类的。  
c 平均值替代----损失信息小，简单高效。  
d 视为缺失值----可以按照处理缺失值的方法来处理。
**去重处理**
 对于重复项的判断，基本思想是“排序与合并”，先将数据集中的记录按一定规则排序，然后通过比较邻近记录是否相似来检测记录是否重复。这里面其实包含了两个操作，一是排序，二是计算相似度。目前在做竞赛过程中主要是用duplicated方法进行判断，然后将重复的样本进行简单的删除处理。  
这块目前看到的博客与国外一些比赛的案例基本都采用直接删除进行处理，没有看到过比较有新意的方法。  
**噪音处理**  
噪音是被测变量的随机误差或者方差，主要区别于离群点。由公式：观测量(Measurement) = 真实数据(True Data) + 噪声 (Noise)。离群点属于观测量，既有可能是真实数据产生的，也有可能是噪声带来的，但是总的来说是和大部分观测量之间有明显不同的观测值。噪音包括错误值或偏离期望的孤立点值，但也不能说噪声点包含离群点，虽然大部分数据挖掘方法都将离群点视为噪声或异常而丢弃。然而，在一些应用（例如：欺诈检测），会针对离群点做离群点分析或异常挖掘。而且有些点在局部是属于离群点，但从全局看是正常的。  
(1) 分箱法：  
分箱方法通过考察数据的“近邻”来光滑有序数据值。这些有序的值被分布到一些“桶”或箱中。由于分箱方法考察近邻的值，因此它进行局部光滑。  
用箱均值光滑：箱中每一个值被箱中的平均值替换。  
用箱中位数平滑：箱中的每一个值被箱中的中位数替换。  
用箱边界平滑：箱中的最大和最小值同样被视为边界。箱中的每一个值被最近的边界值替换。  
一般而言，宽度越大，光滑效果越明显。箱也可以是等宽的，其中每个箱值的区间范围是个常量。分箱也可以作为一种离散化技术使用。  
(2) 回归法  
可以用一个函数拟合数据来光滑数据。线性回归涉及找出拟合两个属性（或变量）的“最佳”直线，使得一个属性能够预测另一个。多线性回归是线性回归的扩展，它涉及多于两个属性，并且数据拟合到一个多维面。使用回归，找出适合数据的数学方程式，能够帮助消除噪声。  

- 调整数值及格式，去掉噪声，不可信值，缺失值较多的字段
- 归一化处理，离散化处理，数据变换
### 特征工程
- 特征提取：通过映射的方法，将高维的属性空间压缩为低维的属性空间，得到最小的属性集，使得数据类的概念分布尽可能地接近使用所有属性的原分布。得到的数据挖掘结果与所有特征参加的数据挖掘结果相近或完全一致。  
-   
**潜在语义分析(LSA)**  
**主成分分析**    
**线性判别分析**
- 特征选择  
 当数据预处理完成后，需要选择有意义的特征输入机器学习的算法和模型进行训练。通常从两方面来选择特征： 1.特征是否发散：如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。2.特征与目标的相关性：与目标相关性高的特征，应当优先选择。  
**特征选择目的**：1减少特征数量、降维，使模型泛化能力更强，减少过拟合。2.增强对特征和特征值直接的理解。   
**特征选择方法**：  
A 过滤型:按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择特征的个数的阈值，选择特征。
sklearn.feature_selection.SelectKBest。  
1.方差选择法：使用方差选择法，先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。  
sklearn.feature_selection.VarianceThreshold   
2.相关系数法:先要计算各个特征对目标值的相关系数以及系数的P值。  
sklearn.feature_selection.SelectKBest  
3.卡方检验：经典的卡方检验是检验定性自变量对定性因变量的相关性。  
4.互信息法：经典的互信息是评价定性自变量对定性因变量的相关性。  
B 包裹型:根据目标函数(通常是预测效果评分)，每次选择若干特征，或者排除若干特征。  
**递归特征消除法**：递归消除特征法使用一个基模型(SVR等)来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。  
sklearn.feature_selection.RFE  
C 嵌入型：先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于过滤法，但是是通过训练来确定特征的优劣。
**基于惩罚项的特征选择法**:使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维。使用feature_selection库的SelectFromModel类结合带L1惩罚项的逻辑回归模型。
**基于树模型的特征选择法**:树模型中GBDT也可用来作为基模型进行特征选择，使用feature_selection库的SelectFromModel类结合GBDT模型。
### 模型选择
###