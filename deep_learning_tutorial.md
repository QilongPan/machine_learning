# 深度学习笔记

[深度学习系列教程](<https://www.zybuluo.com/hanbingtao/note/433855>)

## 感知器(神经元)

神经元和感知器本质上是一样的，只不过我们说感知器的时候，它的激活函数是阶跃函数；而当我们说神经元时，激活函数往往选择为sigmoid函数或者tanh函数等。

阶跃函数：
$$
f(z)=\left\{\begin{matrix}
1 &z>0 \\ 
0 & otherwise
\end{matrix}\right.
$$
神经网络示意图：

![](.\image\神经网络示意图.png)

上图中每个圆圈都是一个神经元，每条线表示神经元之间的连接。我们可以看到，上面的神经元被分成了多层，层与层之间的神经元有连接，而层内之间的神经元没有连接。最左边的层叫做**输入层**，这层负责接收输入数据；最右边的层叫**输出层**，我们可以从这层获取神经网络输出数据。输入层和输出层之间的层叫做**隐藏层**。

隐藏层比较多（大于2）的神经网络叫做深度神经网络。而深度学习，就是使用深层架构（比如，深度神经网络）的机器学习方法。

那么深层网络和浅层网络相比有什么优势呢？简单来说深层网络能够表达力更强。事实上，一个仅有一个隐藏层的神经网络就能拟合任何一个函数，但是它需要很多很多的神经元。而深层网络用少得多的神经元就能拟合同样的函数。也就是为了拟合一个函数，要么使用一个浅而宽的网络，要么使用一个深而窄的网络。而后者往往更节约资源。

深层网络也有劣势，就是它不太容易训练。简单的说，你需要大量的数据，很多的技巧才能训练好一个深层网络。这是个手艺活。

### 感知器定义

![](.\image\感知器.png)

感知器组成部分：

- **输入权值** ：一个感知器可以接收多个输入{x<sub>1</sub>,x<sub>2</sub>,...,x<sub>n</sub>|x<sub>i</sub>∈R}，每个输入上有一个**权值**w<sub>i</sub>∈R，此外还有一个**偏置项**b∈R，就是上图中的w<sub>0</sub>；
- **激活函数f(x)**：感知器的激活函数可以有很多选择，比如sigmoid函数；
- **输出**：感知器的输出由下面这个公式来计算。

$$
y=f(w*x+b)
$$

### 感知器训练

假设损失函数为均方差函数(MSE Mean Square Error)
$$
J(w,b)=\frac{1}{2m}\sum_{i=1}^{m}(a_{i}-y_{i})^{2}\\
J(w,b)=\frac{1}{2m}\sum_{i=1}^{m}(w_{0}x_{0}+w_{1}x_{1}+...+w_{n}x_{n}-y_{i})^{2}
$$
a<sub>i</sub>为预测值，y<sub>i</sub>为实际值。设m=1(训练样本为1条)时
$$
\frac{\partial J(w,b)}{\partial w_{1}}=x_{1}(w_{0}x_{0}+w_{1}x_{1}+...+w_{n}x_{n}-y_{i})
$$
其余参数导数求法相同。

使用梯度下降更新：
$$
w_{i}=w_{i}-\alpha \frac{\partial J(w_{i},b)}{\partial w_{i}}
$$
其中α为学习率。如果损失函数内为y<sub>i</sub>-a<sub>i</sub> ，则上面的梯度下降更新负号应为正号。

## 梯度下降优化算法

![](.\image\梯度下降示例图.png)

函数y=f(x)的极值点就是它的导数f<sup>‘</sup>(x)=0的那个点。因此我们可以通过解方程f<sup>‘</sup>(x)=0,求得函数的极值点。

对于计算机来说，随便选择一个点开始，比如上图的点x<sub>0</sub>。接下来，每次迭代修改的为x<sub>1</sub>,<sub>2</sub>,...，经过数次迭代后最终达到函数最小值点。

你可能要问了，为啥每次修改的值，都能往函数最小值那个方向前进呢？这里的奥秘在于，我们每次都是向函数y=f(x)的**梯度**的**相反方向**来修改。什么是**梯度**呢？翻开大学高数课的课本，我们会发现**梯度**是一个向量，它指向**函数值上升最快**的方向。显然，梯度的反方向当然就是函数值下降最快的方向了。我们每次沿着梯度相反方向去修改的值，当然就能走到函数的最小值附近。之所以是最小值附近而不是最小值那个点，是因为我们每次移动的步长不会那么恰到好处，有可能最后一次迭代走远了越过了最小值那个点。步长的选择是门手艺，如果选择小了，那么就会迭代很多轮才能走到最小值附近；如果选择大了，那可能就会越过最小值很远，收敛不到一个好的点上。

梯度下降算法的公式
$$
X_{new}=X_{old}-\alpha \bigtriangledown f(x)
$$
α为学习率（步长）

## 反向传播

### 神经元

神经元和感知器本质上是一样的，只不过我们说感知器的时候，它的激活函数是**阶跃函数**；而当我们说神经元时，激活函数往往选择为sigmoid函数或tanh函数。如下图所示：

![](.\image\sigmoid神经元.png)

计算一个神经元的输出的方法和计算一个感知器的输出是一样的。假设神经元的输入是向量$$\overrightarrow{x}​$$

 ，权重向量是$$\overrightarrow{w}$$(偏置项是w<sub>0</sub>)，激活函数是sigmoid函数，则其输出y：
$$
y=sigmoid(\vec{w}^{T}*\vec{x})
$$
sigmoid函数定义如下：
$$
sigmoid(x)=\frac{1}{1+e^{-x}}
$$
将其带入前面的式子，得到
$$
y=\frac{1}{1+e^{-\vec{w}^{T}\cdot \vec{x}}}
$$
sigmoid函数是非线性函数，值域是(0,1)。函数图像如下图所示

![](D:\study resource\code\machine_learning\image\sigmoid.jpg)

sigmoid函数的导数是：
$$
令y=sigmoid(x)\\
则y^{'}=y(1-y)
$$
可以看到，sigmoid函数的导数非常有趣，它可以用sigmoid函数自身来表示。这样，一旦计算出sigmoid函数的值，计算它的导数的值就非常方便。

## 优化器

### Batch Gradient Descent(BGD，批量梯度下降)

BGD训练过程中每次迭代使用所有样本来进行梯度的更新。

#### 优点

- 一次迭代是对所有样本进行计算，此时利用矩阵进行操作，实现了并行；
- 由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。当目标函数为凸函数时，BGD一定能够得到全局最优。

#### 缺点

- 当样本数目很大时，每迭代一步都需要对所有样本计算，训练过程会很慢。

### Stochastic Gradient Descent(SGD,随机梯度下降)

SGD训练过程中每次迭代使用一个样本来对参数进行更新。

#### 优点

- 由于不是在全部训练数据上的损失函数，而是在每轮迭代中，随机优化某一条训练数据上的损失函数，这样每一轮参数的更新速度大大加快。

#### 缺点

- 准确度下降。由于即使在目标函数为强凸函数的情况下，SGD仍旧无法做到线性收敛；
- 可能会收敛到局部最优，由于单个样本并不能代表全体样本的趋势；
- 不易于并行实现。

### Mini-Batch Gradient Descent(MBGD,小批量梯度下降)

MBGD训练过程中每次迭代使用**batch size**个样本来对参数进行更新。它是对BGD以及SGD的一个折中办法。

#### 优点

- 通过矩阵运算，每次在一个batch上优化神经网络参数并不会比单个数据慢太多；
- 每次使用一个batch可以大大减小收敛所需要的迭代次数，同时可以使收敛到的结果更加接近梯度下降的效果；
- 可实现并行化。

#### 缺点

- batch size的不当选择可能会带来一些问题。

  batcha size的选择带来的影响：

  - 在合理的范围内，增大batch_size的好处：

    - 内存利用率提高了，大矩阵乘法的并行化效率提高。

    - 跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加快。

    - 在一定范围内，一般来说 batch size 越大，其确定的下降方向越准，引起训练震荡越小。

  - 盲目增大batch size的坏处

    - 内存利用率提高了，但是内存容量可能撑不住了。
    - 跑完一次 epoch（全数据集）所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。
    - batch size 增大到一定程度，其确定的下降方向已经基本不再变化

    

### Momentum

SGD参数更新公式为：  
$$
W:=W-\alpha d_{w}\\
b:=b-\alpha d_{b}
$$
它的梯度更新路线上下波动很大，收敛速度很慢。因此根据这些原因，有人提出了Momentum优化算法，这个是基于SGD的，简单理解就是为了防止波动，它主要是基于梯度的移动指数加权平均来更新参数。引进超参数beta(一般取0.9)。

参数更新公式为：  
$$
V_{dw}=\beta V_{dw}+(1-\beta )dW\\
V_{db}=\beta V_{db}+(1-\beta )db\\
W:=W-\alpha {V_{dw}}\\
b:=b-\alpha {V_{db}}
$$

### Nesterov Momentum

Nesterov Momentum是对Momentum的改进，可以理解为nesterov动量在标准动量方法中添加了一个**校正因子**。

### Root Mean Square Prop(RMSProp)

RMSProp思想与Momentum相似，也用到权重超参数beta（一般取0.999）。

参数更新公式为：
$$
S_{dw}=\beta S_{dw}+(1-\beta )dW^{2}\\
S_{db}=\beta S_{db}+(1-\beta )db^{2}\\
W:=W-\alpha \frac{dW}{\sqrt{S_{dw}}}\\
b:=b-\alpha \frac{db}{\sqrt{S_{db}}}
$$
为了防止分母为0，在分数下加上个特别小的值epsilon，通常选取10^-8。

### Adagrad

大多数优化器训练参数更新过程中都使用了相同的学习率α。Adagrad能够在训练中自动的对learning rate进行调整，对于出现频率较低的参数采用较大的α更新，相反，对于出现频率较高的参数采用较小的α更新。因此，**Adagrad非常适合处理稀疏数据**。

如果是普通的SGD，那么每一时刻梯度的更新公式为：
$$
\Theta _{t+1}=\Theta _{t,i}-\alpha *g_{t,i}
$$
g<sub>t,i</sub>为第t轮第i个参数的梯度。θ<sub>t,i</sub> 为参数值

Adagrad在每轮训练中对每个参数θ<sub>i</sub> 进行更新，参数更新公式为：
$$
\Theta _{t+1,i}=\Theta _{t,i}-\frac{\alpha }{\sqrt{G_{t,ii}+\varepsilon }}*g_{t,i}
$$
G<sub>t</sub>为对角矩阵，大小为D*D。每个对角线位置i,i为对应参数θ<sub>i</sub> 从第一轮到第t轮梯度的平方和。varepsilon 是平滑项，用于避免分母为0，一般取值为10^-8。Adagrad的缺点是在训练的中后期，分母上梯度平方的累加会越来越大，从而梯度趋近于0，使得训练提前结束。

### Adadelta

Adadelta 是 Adagrad 的一个具有更强鲁棒性的的扩展版本，它不是累积所有过去的梯度，而是根据渐变更新的移动窗口调整学习速率。 这样，即使进行了许多更新，Adadelta 仍在继续学习。   

与Adagrad 相比，就是分母的 G 换成了过去的梯度平方的衰减平均值，**指数衰减平均值**。

这个分母相当于**梯度的均方根 root mean squared (RMS)**，在数据统计分析中，将所有值平方求和，求其均值，再开平方，就得到均方根值 。

#### 优点

- 防止学习率衰减或梯度消失等问题的出现。

### Adam

该优化器相当于RMSprop+Momentum。

参数更新公式为：
$$
V_{dw}=\beta _{1} V_{dw}+(1-\beta _{1} )dW\\
V_{db}=\beta _{1} V_{db}+(1-\beta _{1} )db\\
S_{dw}=\beta_{2} S_{dw}+(1-\beta_{2} )dW^{2}\\
S_{db}=\beta_{2} S_{db}+(1-\beta_{2} )db^{2}\\
V_{dW}^{corrected}=\frac{V_{dW}}{1-\beta _{1}^{t}}\\
V_{db}^{corrected}=\frac{V_{db}}{1-\beta _{1}^{t}}\\
S_{dW}^{corrected}=\frac{S_{dW}}{1-\beta _{2}^{t}}\\
S_{db}^{corrected}=\frac{S_{db}}{1-\beta _{2}^{t}}\\
W:=W-\alpha \frac{V_{dW}}{\sqrt{S_{dW}^{corrected}}}\\
b:=b-\alpha \frac{V_{db}}{\sqrt{S_{db}^{corrected}}}
$$
beta1一般为0.9，beta2一般为0.9999。在实际应用中，Adam方法效果良好。与其他自适应学习率算法相比，其收敛速度更快，学习效果更为有效，而且可以纠正其他优化技术中存在的问题，如学习率消失、收敛过慢或是高方差的参数更新导致损失函数波动较大等问题。

## 激活函数

[激活函数详解](<https://zhuanlan.zhihu.com/p/22142013>)

激活函数一般用于神经网络的层与层之间，将上一层的输出转换之后输入到下一层。如果没有激活函数引入的额非线性特性，那么神经网络就只相当于原始感知机的矩阵相乘。  

### Sigmoid

![](.\image\sigmoid.jpg)

sigmoid在定义域内处处可导，且两侧导数逐渐趋近于0。

#### 缺点

- 激活函数计算量大，反向传播求误差梯度时，求导涉及除法；
- 反向传播时，很容易就会出现梯度消失的情况，从而无法完成深层网络的训练；
- Sigmoids函数饱和且kill掉梯度；
- Sigmoids函数收敛缓慢。

### tanh

![](.\image\tanh.jpg)

### Relu

![](.\image\relu.jpg)

#### 优点

- ReLU 得到的 SGD 的收敛速度会比 sigmoid/tanh 快很多。

### 缺点

- 训练的时候很”脆弱”，很容易就”die”了。

### PRelu

![](.\image\PRelu.jpg)

### RReLU

### Maxout

### ELU

![](.\image\elu.jpg)



## 问题及解决方法

### 梯度爆炸与梯度消失

深度神经网络训练过程中，使用了反向传播的方式更新参数，该方式基于的是链式求导。计算每层梯度的时候回设计一些连乘操作，如果网络过深，当连乘的因子大部分小于1时，最后的乘积结果可能趋于0，会导致后面的网络层参数不发生变化，不能继续进行学习（梯度消失）。当连乘的因子大部分大于1，最后的乘积结果可能趋于无穷，会导致后面的网络层参数变化过大，导致Loss值出现震荡，收敛不到最低值的情况（梯度爆炸）。

#### 梯度爆炸解决办法

- 降低学习率

- 梯度裁剪（Gradient Clipping）

  如果梯度特别大，那么将其投影到一个比较小的尺度上。

### 线性与非线性

在数学上可理解为一阶导数为常数的函数为线性函数，一阶导数不为常数的函数为非线性函数。

## 文本分类

[文本分类模型实现](<https://github.com/DX2048/text_classification>)

### FastText

### TextCNN

### TextRNN

### RCNN

### Hierarchical Attention Network

### Seq2seq With Attention

### Dynamic Memory Network

### EntityNetwork:tracking state of the world

### Ensemble models

### Transformer("Attend Is All You Need")

## 如何选择优化算法

如果数据是稀疏的，就用自适用方法，即 Adagrad, Adadelta, RMSprop, Adam。

RMSprop, Adadelta, Adam 在很多情况下的效果是相似的。

Adam 就是在 RMSprop 的基础上加了 bias-correction 和 momentum。

随着梯度变的稀疏，Adam 比 RMSprop 效果会好。

整体来讲，**Adam 是最好的选择**。



## 引用

[零基础入门深度学习](<https://www.zybuluo.com/hanbingtao/note/433855>)