# Reinforcement learning tutorial
**基本原理**：智能体在完成某项任务时，首先通过动作A与周围环境进行交互，在动作A和环境的作用下， 智能体会产生新
的状态， 同时环境会立即给出回报。 如此循环下去， 智能体与环境不断的交互从而产生很多数据。 强化学习算法利用产生的数据修改自身的动作策略，再与环境交互，产生新的数据，并利用新的数据进一步改善自身的行为， 经过数次迭代学习后，智能体能最终学到完成相应任务的最优动作（ 最优策略）。  

## 马尔科夫决策过程
**马尔科夫性**：系统的下一个状态仅与当前状态有关，而与以前的状态无关。  
**马尔科夫过程**：马尔科夫过程是一个二元组(S,P)，且满足：S是有限状态集合，P是状态转移概率。  
**马尔科夫决策过程**：马尔科夫决策过程由元组(S,A,P,R,D)描述，其中：  
S为有限的状态集；  
A为有限的动作集；  
P为状态转移概率；  
R为回报函数；  
D为折扣因子，用来计算累积回报。  
## 基于模型的动态规划方法  
## 基于蒙特卡罗的强化学习方法  
## 基于时间差分的强化学习方法  
Qlearning,Sarsa
## 基于值函数逼近的强化学习方法  

基于动态规划的方法，基于蒙特卡罗的方法和基于时间差分的方法。这些方法有一个基本的前提条件：状态空间和动作空间是离散的，而且状态空间和动作空间不能太大。这些强化学习的基本步骤是先评估值函数，再利用值函数改善当前的策略。若状态空间的维数很大，或者状态空间为连续空间，此时我们需要利用函数逼近的方法表示值函数。当值函数利用函数逼近的方法表示后，可以利用策略迭代和值迭代方法构建强化学习方法。  

DQN,Double DQN(解决过估计问题)，Dueling DQN。
DQN过估计原因：每次求得的Q target都是通过取max得到的，然而我们做的是需要对计算出来的Q值取平均之后才能更新我们的网络，我们都知道 
E(max(X,Y))大于等于max(E(X),E(Y))
可以看出，我们把N（设为32）个Q值先通过取max操作之后，然后求平均(期望)，会比我们先算出32个Q值取了期望之后再max要大。这就是overestimate的原因
DQN与Double DQN的区别在于，在对next时刻的Q值进行选取时，不再使用最大值，而是使用主网络预测出来的next_action进行选取，除此之外算法其余部分完全一样。